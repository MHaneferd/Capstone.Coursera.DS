---
title: "week1"
author: "Martin Haneferd"
date: "10 mars 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

Picked up at: https://www.r-bloggers.com/intro-to-text-analysis-with-r/
and
https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/
and
http://handsondatascience.com/TextMiningO.pdf
and
https://cran.r-project.org/web/views/NaturalLanguageProcessing.html


## Load neccesary libraries

```{r, message=FALSE, cache=FALSE}
library(lsa) # cosine function
library(bigmemory)
library(slam)
library(Rstem)
library(sentiment)
library(plyr)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(qdap)
library(qdapDictionaries)
library(scales)
library(Rgraphviz)
library(magrittr)
library(tm)
library(skmeans)
library(syuzhet) # Sentiment analyzis
library(proxy) # Hierachal clustering
library(igraph) # Fancy graphing
library(topicmodels) #LDA function
library(Matrix)
library(RWeka) 

# install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)
# install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(tm.plugin.sentiment) 

# Helpfunctions and variables:
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(0, .99), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

# Read stopwords:
myStopwords <- scan(file = "./stopwords_en.TXT", what = "character", sep ="\n", encoding = "UTF-8")

myStopwords <- c(myStopwords,"time","year","month","day","week","hour","minute") # Get rid of time variables
skipWords <- function(x){ removeWords(x, stopwords("english"))
      removeWords(x, myStopwords)
}

BigramTokenizer <- function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)


# Set seed so it can be reproduced
set.seed(123)
```

## Download and unpack data
```{r}
# Download the zipped files containing datasets
zipfile="./SwiftKey.zip"
if(!file.exists("./SwiftKey.zip")){
      fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
      download.file(fileUrl, destfile=zipfile, method="libcurl")
      # Unpack files to data directory
      unzip(zipfile)

}

# Look at the file sizes:
file.size(c("./final/en_US/en_US.blogs.TXT","./final/en_US/en_US.news.TXT","./final/en_US/en_US.twitter.TXT"))

# Just keep the english files. Remove the other languages, and the zipfile.
unlink(c("./final/de_DE","./final/fi_FI","./final/ru_RU"), recursive = TRUE, force = TRUE) 
unlink(zipfile)

# Clean up unneccesary values from environment:
rm(fileUrl)
rm(zipfile)
```

## Read the files
```{r, message=FALSE, cache=FALSE}

blogs <- scan(file = "./final/en_US/en_US.blogs.TXT", what = "character", sep ="\n", encoding = "UTF-8", skipNul=TRUE)

news <- scan(file = "./final/en_US/en_US.news.TXT", what = "character", sep ="\n", skipNul=TRUE)

twitter <- scan(file = "./final/en_US/en_US.twitter.TXT", what = "character", sep ="\n", skipNul=TRUE)

```

## Explore blogs data
There is some outlier far out, so I remove them in the histogram plot to get a better feeling of distribution of the major blog content.
```{r}
# Do some cleaning
blogs <- gsub("[^[:ascii:]]|\\’", "\'",blogs, perl=TRUE)
blogs <- tolower(blogs)
# Calculate the number of words.
numWords <- sapply(gregexpr("[[:alpha:]]+", blogs, perl=TRUE), function(x) sum(x > 0))
# Distribution of the numer of words in the Blogs:
summary(numWords) # See that there are some outliers!

numWords.cutted <- remove_outliers(numWords)
summary(numWords.cutted)
hist(numWords.cutted)

numChar <- sapply(blogs, nchar)
# Distribution of char's in the Blogs:
numChar.cutted <- remove_outliers(numChar) # Remove outliers
summary(numChar.cutted)
hist(numChar.cutted)
```
The majority of theblog posts have 10 or less words. I will remove the stop words and clean up the data, and group the blogs in 1-10 words, 11-20 words and 71-80 words, and see if there are any similarities of the top ten words.

### Cleaning the data
```{r}
b.docs <- Corpus(VectorSource(blogs))

tmFuncs <- list(stemDocument,removeNumbers,
                 removePunctuation, skipWords, stripWhitespace,
                 trimws)

# Clean and reduce to important words:
b.docs <- tm_map(b.docs, FUN = tm_reduce, tmFuns = tmFuncs)

# Transforming the tm corpus to vector of strings
blogs.reduce<-sapply(b.docs, as.character)
blogs.reduce[67] #Check
```

### Checking the distribution after reduction
The gap from 10'th to 20'th seems to be reduced. It looks more linear.
```{r}
# count words
numWords <- sapply(gregexpr("[[:alpha:]]+", blogs.reduce, perl=TRUE), function(x) sum(x > 0))

# remove the ones that only have 2 or less words
length(blogs.reduce) # Before
blogs.reduce <- blogs.reduce[numWords > 2]
length(blogs.reduce) #After 

# Do a re-count
numWords <- sapply(gregexpr("[[:alpha:]]+", blogs.reduce, perl=TRUE), function(x) sum(x > 0))

# Distribution of the numer of words in the Blogs after reduction:
numWords.cutted <- remove_outliers(numWords)
summary(numWords.cutted)
hist(numWords.cutted)

```

### Find the high frequencies of the words
```{r}
# Create av Document term matrix
b.dtm <- DocumentTermMatrix(b.docs)

# Find and plot the most used terms in the three sections of blogs
b.freq <- sort(col_sums(b.dtm), decreasing=TRUE)   
wf <- data.frame(word=names(head(b.freq, 15)), freq=head(b.freq, 15))

# Plot
ggplot(wf, aes(x = reorder(word, -freq), y = freq)) + 
      geom_bar(stat = "identity") +
      xlab("Terms (Word)") + 
      ylab("Frequency")
```

### Assosiactions in the blogs for the word love
There did not seem to be any associations to the word love, so I will not get into further investigation of that.
```{r}
# specifying a correlation limit of 0.2 (Not much)
b.assoc <- findAssocs(b.dtm, c("love"), corlimit=0.2) 
b.assoc
```
### Make a word cloud of the 50 most used terms
```{r}
wordcloud(names(head(b.freq, 50)), head(b.freq, 50), colors=brewer.pal(6, "Dark2"))
```
It looks that the majority of the blog posts is about everyday life; work, love, book, people, friends, family, school, hope, etc..

### Sentiment alayzis of the blogs
Lets 1st find out the number and distribution of positive and negative blogs.
```{r}
b.tdm <- TermDocumentMatrix(b.docs)
# Score the blogs in positive or negative
pos.score <- tm_term_score(b.tdm, terms_in_General_Inquirer_categories("Positiv")) # this lists each document with number below

neg.score <- tm_term_score(b.tdm, terms_in_General_Inquirer_categories("Negativ")) 

df.scores <- data.frame(positive = pos.score, negative = neg.score)
df.scores <- transform(df.scores, net = positive - negative)
# Find the spread of the scores
summary(df.scores$net)

# Look at the histogram of the pos/neg blogs
hist(df.scores$net, breaks = 100)
# Looks like there are more negative blogs than positive according to the histogram plot, but check this further

# Find three most positive and three most negative blogs
ord <- df.scores[ order(-df.scores[,3]), ]
most.positive <- as.numeric(rownames(head(ord,3)))
most.negative <- as.numeric(rownames(tail(ord,3)))

# Investigate what they are about:
# The negative:
blogs[most.negative]

# The positive:
blogs[most.positive]

# Count the positive, negative and neutral blogs
sum(  df.scores$net < 0  ) # number of negative blog posts
sum(  df.scores$net > 0  ) # number of positive
sum(  df.scores$net == 0  ) # Number of neutrals

# It is clear that there are more positive than negative blog posts. The histogram above must be adjusted for the neutrals in some ways.
```

And 2nd find out the mood of the blogs:
```{r}
b.sentiment <- get_nrc_sentiment(blogs)
b.df.sentiment <-data.frame(t(b.sentiment))

b.td_new <- data.frame(rowSums(b.df.sentiment))
#The function rowSums computes column sums across rows for each level of a grouping variable.
 
#Transformation and  cleaning
names(b.td_new)[1] <- "count"
b.td_new <- cbind("sentiment" = rownames(b.td_new), b.td_new)
rownames(b.td_new) <- NULL
b.td_new<-b.td_new[1:8,]

#Visualisation
ggplot(b.td_new, aes(x = reorder(sentiment, -count), y = count)) + 
      geom_bar(stat = "identity", aes(fill=sentiment)) +
      xlab("Terms (Word)") + 
      ylab("Frequency")
```
The mood of the blogs looks like it's filled of trust, anticipation and joy.

### Try clustering the data
I will try clustering the data based on the "oil" term
```{r}
b.tdm.love <- TermDocumentMatrix(b.docs, list(dictionary = c("oil")))
#Remove the rows that does not contain the word love
col_freqr <- col_sums(b.tdm.love)
b.tdm.love.only <- b.tdm[, col_freqr>0]
dim(b.tdm.love.only)

# Reduce the matrix to only contain the most used words in the blogs
b.tdm.love.only <- b.tdm.love.only[findFreqTerms(b.tdm.love.only, 300), ]
dim(b.tdm.love.only)

memory.limit(size=60000) # Just chack I have enough memory.
m <- as.matrix(b.tdm.love.only)
d <- dist(m, method="cosine")
#run hierarchical clustering using Ward’s method
hc <- hclust(d, method="ward.D")
plot(hc, hang=-1)
rect.hclust(hc, k=5)
```
Funny. I tought there were more on the oil as in oil-well.. Not as a oil in kitchen-food-making.

### Topic modeling
```{r}
set.seed(123)
b.dtm.x <- DocumentTermMatrix(b.docs, control=list(wordLengths=c(3, 15), bounds = list(global = c(1000,10000),tokenize=BigramTokenizer,weighting = weightTf)))

dim(b.dtm.x)
# Remove zero rows.
freqr.x <- row_sums(b.dtm.x)
b.dtm.x <- b.dtm.x[freqr.x >0,]
dim(b.dtm.x)

#b.dtm.x <- removeSparseTerms(b.dtm.x, 0.99)
myLda <- LDA(b.dtm.x, k=8)

terms(myLda, 10)
```
It is hard to see the difference between the topics. They somewhat fall into same categories.

### Social network analyzis
Just for test.. How is the terms connected...
Reference: https://rdatamining.wordpress.com/2012/05/17/an-example-of-social-network-analysis-with-r-using-package-igraph/
```{r}
### Load Data
b.dtm.x.x <- DocumentTermMatrix(b.docs)
# Concentrate on 20 random picked terms.
b.dtm.x.15 <- b.dtm.x.x[,findFreqTerms(b.dtm.x.x)[7001:7020]]

# Remove zero rows.
freqr.x <- row_sums(b.dtm.x.15)
b.dtm.x.15 <- b.dtm.x.15[freqr.x >0,]
dim(b.dtm.x.15)

### Transform Data into an Adjacency Matrix
# change it to a Boolean matrix
b.dtm.x.15[b.dtm.x.15>=1] <- 1

# transform into a term-term adjacency matrix
b.termMatrix <- sparseMatrix(j=b.dtm.x.15$i, i=b.dtm.x.15$j, x=b.dtm.x.15$v) %*% t(sparseMatrix(j=b.dtm.x.15$i, i=b.dtm.x.15$j, x=b.dtm.x.15$v))

### Build a Graph
dim(b.termMatrix)
# Check sums that the matrix is symmetric:
b.termMatrix[1:20,1:20]
# Now we have built a term-term adjacency matrix, where the rows and columns represents terms, and every entry is the number of co-occurrences of two terms. Next we can build a graph with graph.adjacency() from package igraph.

# build a graph from the above matrix
g <- graph.adjacency(b.termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <-  b.dtm.x.15$dimnames$Terms
V(g)$degree <- degree(g)

### Plot a Graph

# set seed to make the layout reproducible
set.seed(3443)
layout1 <- layout.reingold.tilford(g, circular=T)
plot(g, layout=layout1)
```



## Explore Twitter data
```{r}
# Find number of words in each tweet
numWords <- sapply(gregexpr("[[:alpha:]]+", twitter, perl=TRUE), function(x) sum(x > 0))

# Distribution of the numer of words in the tweets:
summary(numWords)
hist(numWords)
# Looks like the majority of the number of words are around 7-19 words (between 1st and 3rd quantile)

# Find the number of characters
numChar <- sapply(twitter, nchar)
# Distribution of char's in the Tweet:
summary(numChar)
hist(numChar)


```

### Cleaning the data
```{r}
t.docs <- Corpus(VectorSource(twitter))

tmFuncs <- list(stemDocument,removeNumbers,
                 removePunctuation, skipWords, stripWhitespace,
                 trimws)

# Clean and reduce to important words:
t.docs <- tm_map(t.docs, FUN = tm_reduce, tmFuns = tmFuncs)
```

### Find the high frequencies of the words
```{r}
# Create av Document term matrix
t.dtm <- DocumentTermMatrix(t.docs)

# Find and plot the most used terms in the three sections of blogs
t.freq <- sort(col_sums(t.dtm), decreasing=TRUE)   
wf <- data.frame(word=names(head(t.freq, 15)), freq=head(t.freq, 15))

# Plot
ggplot(wf, aes(x = reorder(word, -freq), y = freq)) + 
      geom_bar(stat = "identity") +
      xlab("Terms (Word)") + 
      ylab("Frequency")
```

### Make a word cloud of the 50 most used terms
```{r}
wordcloud(names(head(t.freq, 50)), head(t.freq, 50), colors=brewer.pal(6, "Dark2"))
```
The twitter cloud looks almost like the blog cloud...

### Sentiment alayzis of the twitter messages
Lets 1st find out the number and distribution of positive and negative tweets.
```{r}
t.tdm <- TermDocumentMatrix(t.docs)
# Score the twitter in positive or negative
pos.score <- tm_term_score(t.tdm, terms_in_General_Inquirer_categories("Positiv")) # this lists each document with number below

neg.score <- tm_term_score(t.tdm, terms_in_General_Inquirer_categories("Negativ")) 

df.scores <- data.frame(positive = pos.score, negative = neg.score)
df.scores <- transform(df.scores, net = positive - negative)
# Find the spread of the scores
summary(df.scores$net)

# Look at the histogram of the pos/neg tweets
hist(df.scores$net, breaks = 100)

# Find three most positive and three most negative tweets
ord <- df.scores[ order(-df.scores[,3]), ]
most.positive <- as.numeric(rownames(head(ord,3)))
most.negative <- as.numeric(rownames(tail(ord,3)))

# Investigate what they are about:
# The negative:
twitter[most.negative]

# The positive:
twitter[most.positive]

# Alot of repeated pos/neg words. This is the outliers. 
# Mark: Must remove the extreeme outliers in some ways.

# Count the positive, negative and neutral tweets
sum(  df.scores$net < 0  ) # number of negative tweets
sum(  df.scores$net > 0  ) # number of positive tweets
sum(  df.scores$net == 0  ) # Number of neutrals tweets

# It is clear that there are more neutral tweets.
```
And 2nd find out the mood of the tweets:
```{r}
t.sentiment <- get_nrc_sentiment(twitter)
t.df.sentiment <-data.frame(t(t.sentiment))

t.td_new <- data.frame(rowSums(t.df.sentiment))
#The function rowSums computes column sums across rows for each level of a grouping variable.
 
#Transformation and  cleaning
names(t.td_new)[1] <- "count"
t.td_new <- cbind("sentiment" = rownames(t.td_new), t.td_new)
rownames(t.td_new) <- NULL
t.td_new<-t.td_new[1:8,]

#Visualisation
ggplot(t.td_new, aes(x = reorder(sentiment, -count), y = count)) + 
      geom_bar(stat = "identity", aes(fill=sentiment)) +
      xlab("Terms (Word)") + 
      ylab("Frequency")
```
The mood of the tweets looks like it's filled of trust, anticipation and joy. This is somewhat the same tendency as with the blogs.

### Social network analyzis
How is the terms connected...
```{r}
### Load Data
t.dtm.x.x <- DocumentTermMatrix(t.docs)
# Concentrate on 20 random picked terms.
t.dtm.x.15 <- t.dtm.x.x[,findFreqTerms(t.dtm.x.x)[7001:7020]]

# Remove zero rows.
freqr.t <- row_sums(t.dtm.x.15)
t.dtm.x.15 <- t.dtm.x.15[freqr.t >0,]
dim(t.dtm.x.15)

### Transform Data into an Adjacency Matrix
# change it to a Boolean matrix
t.dtm.x.15[t.dtm.x.15>=1] <- 1

# transform into a term-term adjacency matrix
t.termMatrix <- sparseMatrix(j=t.dtm.x.15$i, i=t.dtm.x.15$j, x=t.dtm.x.15$v) %*% t(sparseMatrix(j=t.dtm.x.15$i, i=t.dtm.x.15$j, x=t.dtm.x.15$v))

### Build a Graph
dim(t.termMatrix)
# Check sums that the matrix is symmetric:
t.termMatrix[1:20,1:20]
# Now we have built a term-term adjacency matrix, where the rows and columns represents terms, and every entry is the number of co-occurrences of two terms. Next we can build a graph with graph.adjacency() from package igraph.

# build a graph from the above matrix
g <- graph.adjacency(t.termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <-  t.dtm.x.15$dimnames$Terms
V(g)$degree <- degree(g)

### Plot a Graph

# set seed to make the layout reproducible
set.seed(3443)
layout1 <- layout.reingold.tilford(g, circular=T)
plot(g, layout=layout1)
```

### Topic modeling
```{r}
set.seed(123)
t.dtm.x <- DocumentTermMatrix(t.docs, control=list(wordLengths=c(3, 15), bounds = list(global = c(1000,10000),tokenize=BigramTokenizer,weighting = weightTf)))

dim(t.dtm.x)
# Remove zero rows.
freqr.t <- row_sums(t.dtm.x)
t.dtm.x <- t.dtm.x[freqr.t >0,]
dim(t.dtm.x)

#This take time!!
library(doParallel)
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)

clusterEvalQ(cluster, {
   library(topicmodels)
})
k <- 8
clusterExport(cluster, c("t.dtm.x", "k"))

myLda <- foreach() %dopar% {
   insideLda <- LDA(t.dtm.x, k = k)
   return(perplexity(insideLda))
}

# myLda <- LDA(t.dtm.x, k=8)

stopCluster(cluster)

terms(myLda, 10)
```

### Find out what the twitter messages is about
```{r}

```


## Extra exploration of the twitter data for the Quiz week 1
```{r}
# Divide the number of lines where the word "love" (all lowercase) occurs by the number of lines the word "hate" (all lowercase) occurs
twitter.low <- tolower(twitter)
sum.love <- sum(grepl("\\<love\\>", twitter.low))
sum.hate <- sum(grepl("\\<hate\\>", twitter.low))
sum.love / sum.hate

# The one tweet in the en_US twitter data set that matches the word "biostats" says what?
twitter.low[grepl("biostats", twitter.low)]

#How many tweets have the exact characters "A computer once beat me at chess, but it was no match for me at kickboxing". (I.e. the line matches those characters exactly.)
sum(grepl("\\<A computer once beat me at chess, but it was no match for me at kickboxing\\>", twitter))
```

## Explore News data
```{r}
# Find number of words in each News
numWords <- sapply(gregexpr("[[:alpha:]]+", news, perl=TRUE), function(x) sum(x > 0))

# Distribution of the numer of words in the News:
summary(numWords)
plot(numWords)

# Find the number of characters
numChar <- sapply(twitter, nchar)
# Distribution of char's in the News:
summary(numChar)
plot(numChar)


```

